{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43dd411f-248e-4b28-8eca-ac1611de036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image splitting and copying complete.\n",
      "Found 1750 files belonging to 3 classes.\n",
      "Found 219 files belonging to 3 classes.\n",
      "Found 219 files belonging to 3 classes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\envs\\rps-cnn\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Datasets prepared and optimized for training.\n"
     ]
    }
   ],
   "source": [
    "# ─── Step 1: Organize raw images into train/val/test folders ───\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define source and target directories\n",
    "SOURCE_DIR = Path(\"../data/raw\")      # original data\n",
    "TARGET_DIR = Path(\"../data/split\")    # new location for train/val/test folders\n",
    "\n",
    "# Create split folders\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    for class_name in [\"rock\", \"paper\", \"scissors\"]:\n",
    "        (TARGET_DIR / split / class_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get all images and labels\n",
    "all_images = list(SOURCE_DIR.glob(\"*/*.png\"))\n",
    "labels = [img.parent.name for img in all_images]\n",
    "\n",
    "# Stratified split: 80% train, 10% val, 10% test\n",
    "train_val_imgs, test_imgs, train_val_labels, test_labels = train_test_split(\n",
    "    all_images, labels, stratify=labels, test_size=0.1, random_state=42\n",
    ")\n",
    "train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n",
    "    train_val_imgs, train_val_labels, stratify=train_val_labels, test_size=0.1111, random_state=42\n",
    ") \n",
    "\n",
    "# Helper function to copy images into new folders\n",
    "def copy_images(image_paths, labels, split_name):\n",
    "    for img_path, label in zip(image_paths, labels):\n",
    "        dst = TARGET_DIR / split_name / label / img_path.name\n",
    "        shutil.copy(img_path, dst)\n",
    "\n",
    "# Perform the actual copying\n",
    "copy_images(train_imgs, train_labels, \"train\")\n",
    "copy_images(val_imgs, val_labels, \"val\")\n",
    "copy_images(test_imgs, test_labels, \"test\")\n",
    "\n",
    "print(\"Image splitting and copying complete.\")\n",
    "\n",
    "# ─── Step 2: Setup Image Parameters and Load with TensorFlow ───\n",
    "import tensorflow as tf\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = (150, 150)\n",
    "BATCH_SIZE = 32\n",
    "classes = [\"rock\", \"paper\", \"scissors\"]\n",
    "\n",
    "# Base directory for the new dataset\n",
    "DATA_DIR = TARGET_DIR\n",
    "\n",
    "# Load datasets using image_dataset_from_directory\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR / \"train\",\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR / \"val\",\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR / \"test\",\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ─── Step 3: Normalize Pixel Values ───\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Normalization layer: rescales [0,255] → [0,1]\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "# Apply normalization via map (this keeps the datasets lazy/efficient)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds   = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds  = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# ─── Step 4: Dataset Performance Optimizations ───\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds  = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"Datasets prepared and optimized for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fcde9-7cce-42fe-97a2-fe2239f764e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
